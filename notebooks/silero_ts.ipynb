{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "22456d11",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/timur.bikbulatov/personal/aa_on_vad\n",
      "/home/timur.bikbulatov/personal/aa_on_vad\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/timur.bikbulatov/miniconda3/envs/aaml/lib/python3.11/site-packages/IPython/core/magics/osm.py:417: UserWarning: This is now an optional IPython functionality, setting dhist requires you to install the `pickleshare` library.\n",
      "  self.shell.db['dhist'] = compress_dhist(dhist)[-100:]\n"
     ]
    }
   ],
   "source": [
    "%cd ../\n",
    "!pwd "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "16f4df3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.graph_objects as go\n",
    "import numpy as np\n",
    "\n",
    "def plot(y:list):\n",
    "    trace = []\n",
    "    colors = [\n",
    "        'Blue',\n",
    "        'Orange',\n",
    "        'Green',\n",
    "        'Red',\n",
    "        'Purple',\n",
    "        'Magenta',\n",
    "        'Cyan',\n",
    "        'Brown',\n",
    "        'Pink',\n",
    "        'Lime',\n",
    "        'Yellow',\n",
    "        'Teal',\n",
    "        'Olive',\n",
    "        'Navy',\n",
    "        'Maroon',\n",
    "        'Coral',\n",
    "        'Gold',\n",
    "        'Indigo',\n",
    "        'Turquoise',\n",
    "        'Lavender',\n",
    "        'Mint',\n",
    "        'Silver',\n",
    "        ]\n",
    "    for ik, y_ in enumerate(y):\n",
    "        trace.append(go.Scatter(x=np.arange(len(y_)), y=y_, mode='lines', name=f'arg # {ik + 1}', line=dict(color=colors[ik])))\n",
    "\n",
    "    # Combining both traces into one figure\n",
    "    fig = go.Figure(data=trace)\n",
    "\n",
    "    # Setting the layout\n",
    "    fig.update_layout(\n",
    "        title='Two Line Charts on One Plot',\n",
    "        xaxis_title='X-axis',\n",
    "        yaxis_title='Y-axis',\n",
    "        showlegend=True\n",
    "    )\n",
    "\n",
    "    # Display the plot\n",
    "    fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "initial_id",
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5dd2a364",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in /home/timur.bikbulatov/.cache/torch/hub/snakers4_silero-vad_master\n"
     ]
    }
   ],
   "source": [
    "torch.set_num_threads(1)\n",
    "\n",
    "model, utils = torch.hub.load(repo_or_dir='snakers4/silero-vad', model='silero_vad')\n",
    "(get_speech_timestamps, _, read_audio, _, _) = utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f4454423",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'start': 0.3, 'end': 2.1}, {'start': 3.0, 'end': 5.2}, {'start': 6.7, 'end': 7.3}, {'start': 7.5, 'end': 8.2}, {'start': 12.0, 'end': 15.9}, {'start': 17.2, 'end': 20.4}, {'start': 20.6, 'end': 23.4}, {'start': 23.9, 'end': 24.6}, {'start': 24.9, 'end': 28.5}, {'start': 28.9, 'end': 34.9}, {'start': 35.1, 'end': 35.8}, {'start': 36.0, 'end': 42.5}, {'start': 42.9, 'end': 44.3}, {'start': 44.5, 'end': 54.3}, {'start': 54.4, 'end': 56.6}, {'start': 56.7, 'end': 57.2}, {'start': 57.3, 'end': 61.8}, {'start': 61.9, 'end': 65.2}, {'start': 65.7, 'end': 67.6}, {'start': 68.5, 'end': 70.4}, {'start': 73.9, 'end': 78.6}, {'start': 79.4, 'end': 84.3}]\n"
     ]
    }
   ],
   "source": [
    "wav = read_audio('datasets/speech16.wav')\n",
    "speech_timestamps = get_speech_timestamps(\n",
    "  wav,\n",
    "  model,\n",
    "  return_seconds=True,  # Return speech timestamps in seconds (default is samples)\n",
    ")\n",
    "\n",
    "print(speech_timestamps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "14e5e235",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_2301002/1298232958.py:1: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  wav1 = torch.tensor(wav, requires_grad=True)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([0., 0., 0.,  ..., 0., 0., 0.], requires_grad=True)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wav1 = torch.tensor(wav, requires_grad=True)\n",
    "wav1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ef93a92d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         ...,\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.]]],\n",
       "       grad_fn=<DifferentiableGraphBackward>)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "o1 = model._model.stft(wav1.reshape(1, -1))\n",
    "o1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "1e0fdfc1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[0.2081, 0.0000, 0.0032,  ..., 0.0000, 0.0000, 0.5323],\n",
       "         [0.0697, 1.6500, 1.6210,  ..., 1.5718, 1.4964, 0.0000],\n",
       "         [0.2396, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 1.9742],\n",
       "         ...,\n",
       "         [0.3942, 1.0355, 1.2398,  ..., 1.0459, 1.0875, 1.0967],\n",
       "         [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
       "         [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000]]],\n",
       "       grad_fn=<ReluBackward0>)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "o2 = model._model.encoder(o1)\n",
    "o2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "7d2ba660",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 128, 2731])"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "o2.squeeze(-1).size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "0e540a2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "state = torch.zeros(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "62d154d6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.0123]], grad_fn=<SigmoidBackward0>)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "o3, state = model._model.decoder(o2[0,:, 0], state)\n",
    "o3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "c77cd925",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 128, 2731])"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "o2.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "439fa58a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchaudio\n",
    "\n",
    "def process_audio_internal(model, waveform, sr=16000):\n",
    "    chunk_size = 512\n",
    "    \n",
    "    context_size = 32\n",
    "    total_pad = chunk_size - (waveform.shape[1] % chunk_size)\n",
    "    padded_waveform = torch.nn.functional.pad(waveform, (context_size, total_pad + context_size))\n",
    "    stft_features = model._model.stft(padded_waveform)\n",
    "    \n",
    "    encoder_features = model._model.encoder(stft_features)\n",
    "    \n",
    "    state = torch.zeros(0)\n",
    "    \n",
    "    predictions = []\n",
    "    print(f\"STFT feature shape: {stft_features.shape}\")\n",
    "    print(f\"Encoder feature shape: {encoder_features.shape}\")\n",
    "    for i in range(encoder_features.shape[2]):\n",
    "        current_features = encoder_features[:, :, i:i+1]\n",
    "        \n",
    "        pred, state = model._model.decoder(current_features, state)\n",
    "        predictions.append(pred)\n",
    "    \n",
    "    all_predictions = torch.cat(predictions, dim=2)\n",
    "    valid_predictions = all_predictions[:, :, context_size//chunk_size:-context_size//chunk_size]\n",
    "    \n",
    "    return valid_predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "e7c45734",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "STFT feature shape: torch.Size([1, 129, 10924])\n",
      "Encoder feature shape: torch.Size([1, 128, 2731])\n",
      "Final predictions shape: torch.Size([1, 1, 2730])\n",
      "Number of speech windows detected: 597\n"
     ]
    }
   ],
   "source": [
    "def process_file(audio_path):\n",
    "    waveform, source_sr = torchaudio.load(audio_path)\n",
    "    if waveform.shape[0] > 1:\n",
    "        waveform = waveform.mean(dim=0, keepdim=True)\n",
    "    \n",
    "    if source_sr != 16000:\n",
    "        resampler = torchaudio.transforms.Resample(source_sr, 16000)\n",
    "        waveform = resampler(waveform)\n",
    "    \n",
    "    predictions = process_audio_internal(model, waveform)\n",
    "    \n",
    "    speech_mask = (predictions[0, 0] > 0.5).cpu().numpy()\n",
    "    \n",
    "    \n",
    "    print(f\"Final predictions shape: {predictions.shape}\")\n",
    "    print(f\"Number of speech windows detected: {speech_mask.sum()}\")\n",
    "    \n",
    "    return predictions, speech_mask\n",
    "\n",
    "# Example usage\n",
    "predictions, speech_mask = process_file(\"datasets/speech16.wav\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "9ea0d0f6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "512.0978021978023"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wav.size()[-1] / 2730"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "23f2cc40",
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = np.repeat(predictions.detach().numpy(), 512)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30819ea6",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot([wav.numpy(),preds])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "ee7151f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "STFT feature shape: torch.Size([1, 129, 11988])\n",
      "Encoder feature shape: torch.Size([1, 128, 2997])\n",
      "Final predictions shape: torch.Size([1, 1, 2996])\n",
      "Number of speech windows detected: 0\n"
     ]
    }
   ],
   "source": [
    "predictions, speech_mask = process_file(\"datasets/woaini.wav\")\n",
    "preds = np.repeat(predictions.detach().numpy(), 512)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "952d1cff",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1534375])"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wav = read_audio('datasets/woaini.wav')\n",
    "wav.size()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "22de23f1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "34.793083900226755"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "1534375 / 44100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "d08ef0c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import soundfile as sf\n",
    "\n",
    "w, sr = sf.read('datasets/woaini.wav')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "3595c41f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "44100"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "c3738d31",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4229120, 2)"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a48dda93",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "aaml",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
