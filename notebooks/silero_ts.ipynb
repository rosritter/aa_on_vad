{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "22456d11",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/timur.bikbulatov/personal/aa_on_vad\n",
      "/home/timur.bikbulatov/personal/aa_on_vad\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/timur.bikbulatov/miniconda3/envs/aaml/lib/python3.11/site-packages/IPython/core/magics/osm.py:417: UserWarning: This is now an optional IPython functionality, setting dhist requires you to install the `pickleshare` library.\n",
      "  self.shell.db['dhist'] = compress_dhist(dhist)[-100:]\n"
     ]
    }
   ],
   "source": [
    "%cd ../\n",
    "!pwd "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "16f4df3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.graph_objects as go\n",
    "import numpy as np\n",
    "\n",
    "def plot(y:list):\n",
    "    trace = []\n",
    "    colors = [\n",
    "        'Blue',\n",
    "        'Orange',\n",
    "        'Green',\n",
    "        'Red',\n",
    "        'Purple',\n",
    "        'Magenta',\n",
    "        'Cyan',\n",
    "        'Brown',\n",
    "        'Pink',\n",
    "        'Lime',\n",
    "        'Yellow',\n",
    "        'Teal',\n",
    "        'Olive',\n",
    "        'Navy',\n",
    "        'Maroon',\n",
    "        'Coral',\n",
    "        'Gold',\n",
    "        'Indigo',\n",
    "        'Turquoise',\n",
    "        'Lavender',\n",
    "        'Mint',\n",
    "        'Silver',\n",
    "        ]\n",
    "    for ik, y_ in enumerate(y):\n",
    "        trace.append(go.Scatter(x=np.arange(len(y_)), y=y_, mode='lines', name=f'arg # {ik + 1}', line=dict(color=colors[ik])))\n",
    "\n",
    "    # Combining both traces into one figure\n",
    "    fig = go.Figure(data=trace)\n",
    "\n",
    "    # Setting the layout\n",
    "    fig.update_layout(\n",
    "        title='Two Line Charts on One Plot',\n",
    "        xaxis_title='X-axis',\n",
    "        yaxis_title='Y-axis',\n",
    "        showlegend=True\n",
    "    )\n",
    "\n",
    "    # Display the plot\n",
    "    fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5ddd2ba3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import IPython\n",
    "from functools import partial\n",
    "play = partial(IPython.display.Audio,\n",
    "               rate=16000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "initial_id",
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5dd2a364",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in /home/timur.bikbulatov/.cache/torch/hub/snakers4_silero-vad_master\n"
     ]
    }
   ],
   "source": [
    "torch.set_num_threads(1)\n",
    "\n",
    "model, utils = torch.hub.load(repo_or_dir='snakers4/silero-vad', model='silero_vad')\n",
    "(get_speech_timestamps, _, read_audio, _, _) = utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f4454423",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'start': 0.3, 'end': 2.1}, {'start': 3.0, 'end': 5.2}, {'start': 6.7, 'end': 7.3}, {'start': 7.5, 'end': 8.2}, {'start': 12.0, 'end': 15.9}, {'start': 17.2, 'end': 20.4}, {'start': 20.6, 'end': 23.4}, {'start': 23.9, 'end': 24.6}, {'start': 24.9, 'end': 28.5}, {'start': 28.9, 'end': 34.9}, {'start': 35.1, 'end': 35.8}, {'start': 36.0, 'end': 42.5}, {'start': 42.9, 'end': 44.3}, {'start': 44.5, 'end': 54.3}, {'start': 54.4, 'end': 56.6}, {'start': 56.7, 'end': 57.2}, {'start': 57.3, 'end': 61.8}, {'start': 61.9, 'end': 65.2}, {'start': 65.7, 'end': 67.6}, {'start': 68.5, 'end': 70.4}, {'start': 73.9, 'end': 78.6}, {'start': 79.4, 'end': 84.3}]\n"
     ]
    }
   ],
   "source": [
    "wav = read_audio('datasets/speech16.wav')\n",
    "speech_timestamps = get_speech_timestamps(\n",
    "  wav,\n",
    "  model,\n",
    "  return_seconds=True,  # Return speech timestamps in seconds (default is samples)\n",
    ")\n",
    "\n",
    "print(speech_timestamps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "14e5e235",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_2353896/1298232958.py:1: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  wav1 = torch.tensor(wav, requires_grad=True)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([0., 0., 0.,  ..., 0., 0., 0.], requires_grad=True)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wav1 = torch.tensor(wav, requires_grad=True)\n",
    "wav1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ef93a92d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         ...,\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.]]],\n",
       "       grad_fn=<DifferentiableGraphBackward>)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "o1 = model._model.stft(wav1.reshape(1, -1))\n",
    "o1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "1e0fdfc1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[0.2081, 0.0000, 0.0032,  ..., 0.0000, 0.0000, 0.5323],\n",
       "         [0.0697, 1.6500, 1.6210,  ..., 1.5718, 1.4964, 0.0000],\n",
       "         [0.2396, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 1.9742],\n",
       "         ...,\n",
       "         [0.3942, 1.0355, 1.2398,  ..., 1.0459, 1.0875, 1.0967],\n",
       "         [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
       "         [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000]]],\n",
       "       grad_fn=<ReluBackward0>)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "o2 = model._model.encoder(o1)\n",
    "o2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "7d2ba660",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 128, 2731])"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "o2.squeeze(-1).size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "0e540a2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "state = torch.zeros(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "62d154d6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.0123]], grad_fn=<SigmoidBackward0>)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "o3, state = model._model.decoder(o2[0,:, 0], state)\n",
    "o3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "c77cd925",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 128, 2731])"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "o2.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "439fa58a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchaudio\n",
    "\n",
    "def process_audio_internal(model, waveform, sr=16000):\n",
    "    chunk_size = 512\n",
    "    \n",
    "    context_size = 32\n",
    "    total_pad = chunk_size - (waveform.shape[1] % chunk_size)\n",
    "    padded_waveform = torch.nn.functional.pad(waveform, (context_size, total_pad + context_size))\n",
    "    stft_features = model._model.stft(padded_waveform)\n",
    "    \n",
    "    encoder_features = model._model.encoder(stft_features)\n",
    "    \n",
    "    state = torch.zeros(0)\n",
    "    \n",
    "    predictions = []\n",
    "    print(f\"STFT feature shape: {stft_features.shape}\")\n",
    "    print(f\"Encoder feature shape: {encoder_features.shape}\")\n",
    "    for i in range(encoder_features.shape[2]):\n",
    "        current_features = encoder_features[:, :, i:i+1]\n",
    "        \n",
    "        pred, state = model._model.decoder(current_features, state)\n",
    "        predictions.append(pred)\n",
    "    \n",
    "    all_predictions = torch.cat(predictions, dim=2)\n",
    "    valid_predictions = all_predictions[:, :, context_size//chunk_size:-context_size//chunk_size]\n",
    "    \n",
    "    return valid_predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "e7c45734",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_file(audio_path):\n",
    "    waveform, source_sr = torchaudio.load(audio_path)\n",
    "    if waveform.shape[0] > 1:\n",
    "        waveform = waveform.mean(dim=0, keepdim=True)\n",
    "    \n",
    "    if source_sr != 16000:\n",
    "        resampler = torchaudio.transforms.Resample(source_sr, 16000)\n",
    "        waveform = resampler(waveform)\n",
    "    \n",
    "    predictions = process_audio_internal(model, waveform)\n",
    "    \n",
    "    speech_mask = (predictions[0, 0] > 0.3).cpu().numpy()\n",
    "    \n",
    "    \n",
    "    print(f\"Final predictions shape: {predictions.shape}\")\n",
    "    print(f\"Number of speech windows detected: {speech_mask.sum()}\")\n",
    "    \n",
    "    return predictions, speech_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "0bd03ce0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "STFT feature shape: torch.Size([1, 129, 10924])\n",
      "Encoder feature shape: torch.Size([1, 128, 2731])\n",
      "Final predictions shape: torch.Size([1, 1, 2730])\n",
      "Number of speech windows detected: 943\n"
     ]
    }
   ],
   "source": [
    "predictions, speech_mask = process_file(\"datasets/speech16.wav\")\n",
    "wav = read_audio('datasets/speech16.wav')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "b17a3d7e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[0.0123, 0.0085, 0.0177,  ..., 0.0155, 0.0154, 0.0153]]],\n",
       "       grad_fn=<SliceBackward0>)"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "84c1b990",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'start': 0.3, 'end': 2.1},\n",
       " {'start': 3.0, 'end': 5.2},\n",
       " {'start': 6.7, 'end': 7.3},\n",
       " {'start': 7.5, 'end': 8.2},\n",
       " {'start': 12.0, 'end': 15.9},\n",
       " {'start': 17.2, 'end': 20.4},\n",
       " {'start': 20.6, 'end': 23.4},\n",
       " {'start': 23.9, 'end': 24.6},\n",
       " {'start': 24.9, 'end': 28.5},\n",
       " {'start': 28.9, 'end': 34.9},\n",
       " {'start': 35.1, 'end': 35.8},\n",
       " {'start': 36.0, 'end': 42.5},\n",
       " {'start': 42.9, 'end': 44.3},\n",
       " {'start': 44.5, 'end': 54.3},\n",
       " {'start': 54.4, 'end': 56.6},\n",
       " {'start': 56.7, 'end': 57.2},\n",
       " {'start': 57.3, 'end': 61.8},\n",
       " {'start': 61.9, 'end': 65.2},\n",
       " {'start': 65.7, 'end': 67.6},\n",
       " {'start': 68.5, 'end': 70.4},\n",
       " {'start': 73.9, 'end': 78.6},\n",
       " {'start': 79.4, 'end': 84.3}]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "speech_timestamps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "fb18f101",
   "metadata": {},
   "outputs": [],
   "source": [
    "timestamps = np.zeros(wav.size()[0])\n",
    "\n",
    "for t in speech_timestamps:\n",
    "    timestamps[int(t['start']*16000):int(t['end']*16000)] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "33e6ceeb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2730,)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.array(speech_mask).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "9ea0d0f6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "512.0978021978023"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wav.size()[-1] / 2730"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "23f2cc40",
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = np.repeat(predictions.detach().numpy(), 512)\n",
    "smask = np.repeat(np.array(speech_mask), 512)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30819ea6",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot([wav.numpy()[:300000],smask[:300000].astype(int), timestamps[:300000], preds[:300000]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "ee7151f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "STFT feature shape: torch.Size([1, 129, 11988])\n",
      "Encoder feature shape: torch.Size([1, 128, 2997])\n",
      "Final predictions shape: torch.Size([1, 1, 2996])\n",
      "Number of speech windows detected: 0\n"
     ]
    }
   ],
   "source": [
    "predictions, speech_mask = process_file(\"datasets/woaini.wav\")\n",
    "preds = np.repeat(predictions.detach().numpy(), 512)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "952d1cff",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1534375])"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wav = read_audio('datasets/woaini.wav')\n",
    "wav.size()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22de23f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot([wav[:300000], preds[:300000]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "d08ef0c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import soundfile as sf\n",
    "\n",
    "w, sr = sf.read('datasets/woaini.wav')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "3595c41f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "44100"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "c3738d31",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4229120, 2)"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "a48dda93",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1534374.603174603"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "4229120 / 44100 * 16000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15d59237",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "\n",
    "@torch.no_grad()\n",
    "def get_speech_timestamps(audio: torch.Tensor,\n",
    "                          model,\n",
    "                          threshold: float = 0.5,\n",
    "                          sampling_rate: int = 16000,\n",
    "                          min_speech_duration_ms: int = 250,\n",
    "                          max_speech_duration_s: float = float('inf'),\n",
    "                          min_silence_duration_ms: int = 100,\n",
    "                          speech_pad_ms: int = 30,\n",
    "                          return_seconds: bool = False,\n",
    "                          visualize_probs: bool = False,\n",
    "                          neg_threshold: float = None,\n",
    "                          window_size_samples: int = 512,):\n",
    "\n",
    "    window_size_samples = 512 if sampling_rate == 16000 else 256\n",
    "\n",
    "    # model.reset_states()\n",
    "    min_speech_samples = sampling_rate * min_speech_duration_ms / 1000\n",
    "    speech_pad_samples = sampling_rate * speech_pad_ms / 1000\n",
    "    max_speech_samples = sampling_rate * max_speech_duration_s - window_size_samples - 2 * speech_pad_samples\n",
    "    min_silence_samples = sampling_rate * min_silence_duration_ms / 1000\n",
    "    min_silence_samples_at_max_speech = sampling_rate * 98 / 1000\n",
    "\n",
    "    audio_length_samples = len(audio)\n",
    "    state = torch.zeros(0)\n",
    "    speech_probs = []\n",
    "    for current_start_sample in range(0, audio_length_samples, window_size_samples):\n",
    "        chunk = audio[current_start_sample: current_start_sample + window_size_samples]\n",
    "        if len(chunk) < window_size_samples:\n",
    "            chunk = torch.nn.functional.pad(chunk, (0, int(window_size_samples - len(chunk))))\n",
    "        # speech_prob = model(chunk, sampling_rate).item()\n",
    "        stft_features = model._model.stft(chunk)\n",
    "\n",
    "        speech_probs.append(speech_prob)\n",
    "\n",
    "    return speech_prob\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99399ec1",
   "metadata": {},
   "outputs": [],
   "source": [
    "chunk_size = 512\n",
    "    \n",
    "    context_size = 32\n",
    "    total_pad = chunk_size - (waveform.shape[1] % chunk_size)\n",
    "    padded_waveform = torch.nn.functional.pad(waveform, (context_size, total_pad + context_size))\n",
    "    stft_features = model._model.stft(padded_waveform)\n",
    "    \n",
    "    encoder_features = model._model.encoder(stft_features)\n",
    "    \n",
    "    state = torch.zeros(0)\n",
    "    \n",
    "    predictions = []\n",
    "    print(f\"STFT feature shape: {stft_features.shape}\")\n",
    "    print(f\"Encoder feature shape: {encoder_features.shape}\")\n",
    "    for i in range(encoder_features.shape[2]):\n",
    "        current_features = encoder_features[:, :, i:i+1]\n",
    "        \n",
    "        pred, state = model._model.decoder(current_features, state)\n",
    "        predictions.append(pred)\n",
    "    \n",
    "    all_predictions = torch.cat(predictions, dim=2)\n",
    "    valid_predictions = all_predictions[:, :, context_size//chunk_size:-context_size//chunk_size]\n",
    "    \n",
    "    return valid_predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "c0b4c104",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 1, 128])"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.reset_states()\n",
    "stft = model._model.stft(wav[:512].reshape(1,-1))\n",
    "encoder_out = model._model.encoder(stft)\n",
    "state, output = model._model.decoder(encoder_out, torch.zeros(0))\n",
    "model._model.decoder.decoder(output[0].unsqueeze(-1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "58eb72f3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RecursiveScriptModule(original_name=LSTMCell)"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model._model.decoder.rnn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "b7f68d57",
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "Dimension out of range (expected to be in range of [-2, 1], but got 2)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[84], line 5\u001b[0m\n\u001b[1;32m      3\u001b[0m encoder_out \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39m_model\u001b[38;5;241m.\u001b[39mencoder(stft)\n\u001b[1;32m      4\u001b[0m h \u001b[38;5;241m=\u001b[39m c \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mzeros(encoder_out\u001b[38;5;241m.\u001b[39msize(\u001b[38;5;241m0\u001b[39m), \u001b[38;5;241m128\u001b[39m)  \u001b[38;5;66;03m# Hidden and cell states for LSTM\u001b[39;00m\n\u001b[0;32m----> 5\u001b[0m decoder_input \u001b[38;5;241m=\u001b[39m encoder_out\u001b[38;5;241m.\u001b[39msqueeze(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\u001b[38;5;241m.\u001b[39mtranspose(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m2\u001b[39m)\n",
      "\u001b[0;31mIndexError\u001b[0m: Dimension out of range (expected to be in range of [-2, 1], but got 2)"
     ]
    }
   ],
   "source": [
    "model.reset_states()\n",
    "stft = model._model.stft(wav[:512].reshape(1,-1))\n",
    "encoder_out = model._model.encoder(stft)\n",
    "h = c = torch.zeros(encoder_out.size(0), 128)  # Hidden and cell states for LSTM\n",
    "decoder_input = encoder_out.squeeze(-1)\n",
    "x = h.unsqueeze(-1).float()  # Shape: [batch_size, features, 1]\n",
    "out = decoder.decoder(x)  # Shape: [batch_size, 1, 1]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "f5a04782",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'RecursiveScriptModule' object has no attribute 'reset_states'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[79], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m model\u001b[38;5;241m.\u001b[39m_model\u001b[38;5;241m.\u001b[39mreset_states()\n",
      "File \u001b[0;32m~/miniconda3/envs/aaml/lib/python3.11/site-packages/torch/jit/_script.py:826\u001b[0m, in \u001b[0;36mRecursiveScriptModule.__getattr__\u001b[0;34m(self, attr)\u001b[0m\n\u001b[1;32m    823\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__dict__\u001b[39m[attr] \u001b[38;5;241m=\u001b[39m script_method\n\u001b[1;32m    824\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m script_method\n\u001b[0;32m--> 826\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__getattr__\u001b[39m(attr)\n",
      "File \u001b[0;32m~/miniconda3/envs/aaml/lib/python3.11/site-packages/torch/jit/_script.py:533\u001b[0m, in \u001b[0;36mScriptModule.__getattr__\u001b[0;34m(self, attr)\u001b[0m\n\u001b[1;32m    531\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__getattr__\u001b[39m(\u001b[38;5;28mself\u001b[39m, attr):\n\u001b[1;32m    532\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_actual_script_module\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__dict__\u001b[39m:\n\u001b[0;32m--> 533\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__getattr__\u001b[39m(attr)\n\u001b[1;32m    534\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mgetattr\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_actual_script_module, attr)\n",
      "File \u001b[0;32m~/miniconda3/envs/aaml/lib/python3.11/site-packages/torch/nn/modules/module.py:1931\u001b[0m, in \u001b[0;36mModule.__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m   1929\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01min\u001b[39;00m modules:\n\u001b[1;32m   1930\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m modules[name]\n\u001b[0;32m-> 1931\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAttributeError\u001b[39;00m(\n\u001b[1;32m   1932\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtype\u001b[39m(\u001b[38;5;28mself\u001b[39m)\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m object has no attribute \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1933\u001b[0m )\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'RecursiveScriptModule' object has no attribute 'reset_states'"
     ]
    }
   ],
   "source": [
    "model._model.reset_states()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "025503d2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 128, 1])"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output.reshape(2,-1).unsqueeze(-1).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "94422046",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.0120]], grad_fn=<UnsqueezeBackward0>)"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.reset_states()\n",
    "model(wav[:512].reshape(1,-1), 16000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "81230843",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "stft_features shape: torch.Size([1, 129, 10921])\n",
      "encoder_out shape: torch.Size([1, 128, 2731])\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'RecursiveScriptModule' object has no attribute 'hidden_size'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[86], line 14\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[38;5;66;03m# Initialize decoder state\u001b[39;00m\n\u001b[1;32m     13\u001b[0m decoder \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39m_model\u001b[38;5;241m.\u001b[39mdecoder\n\u001b[0;32m---> 14\u001b[0m state \u001b[38;5;241m=\u001b[39m (torch\u001b[38;5;241m.\u001b[39mzeros(encoder_out\u001b[38;5;241m.\u001b[39msize(\u001b[38;5;241m0\u001b[39m), decoder\u001b[38;5;241m.\u001b[39mhidden_size),\n\u001b[1;32m     15\u001b[0m          torch\u001b[38;5;241m.\u001b[39mzeros(encoder_out\u001b[38;5;241m.\u001b[39msize(\u001b[38;5;241m0\u001b[39m), decoder\u001b[38;5;241m.\u001b[39mhidden_size))\n\u001b[1;32m     16\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDecoder hidden size:\u001b[39m\u001b[38;5;124m\"\u001b[39m, decoder\u001b[38;5;241m.\u001b[39mhidden_size)\n\u001b[1;32m     18\u001b[0m \u001b[38;5;66;03m# Process through the decoder\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/aaml/lib/python3.11/site-packages/torch/jit/_script.py:826\u001b[0m, in \u001b[0;36mRecursiveScriptModule.__getattr__\u001b[0;34m(self, attr)\u001b[0m\n\u001b[1;32m    823\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__dict__\u001b[39m[attr] \u001b[38;5;241m=\u001b[39m script_method\n\u001b[1;32m    824\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m script_method\n\u001b[0;32m--> 826\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__getattr__\u001b[39m(attr)\n",
      "File \u001b[0;32m~/miniconda3/envs/aaml/lib/python3.11/site-packages/torch/jit/_script.py:533\u001b[0m, in \u001b[0;36mScriptModule.__getattr__\u001b[0;34m(self, attr)\u001b[0m\n\u001b[1;32m    531\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__getattr__\u001b[39m(\u001b[38;5;28mself\u001b[39m, attr):\n\u001b[1;32m    532\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_actual_script_module\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__dict__\u001b[39m:\n\u001b[0;32m--> 533\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__getattr__\u001b[39m(attr)\n\u001b[1;32m    534\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mgetattr\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_actual_script_module, attr)\n",
      "File \u001b[0;32m~/miniconda3/envs/aaml/lib/python3.11/site-packages/torch/nn/modules/module.py:1931\u001b[0m, in \u001b[0;36mModule.__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m   1929\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01min\u001b[39;00m modules:\n\u001b[1;32m   1930\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m modules[name]\n\u001b[0;32m-> 1931\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAttributeError\u001b[39;00m(\n\u001b[1;32m   1932\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtype\u001b[39m(\u001b[38;5;28mself\u001b[39m)\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m object has no attribute \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1933\u001b[0m )\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'RecursiveScriptModule' object has no attribute 'hidden_size'"
     ]
    }
   ],
   "source": [
    "# Prepare the audio input\n",
    "padded_waveform = wav.reshape(1, -1)  # Shape: [batch_size, time]\n",
    "\n",
    "# Extract STFT features\n",
    "stft_features = model._model.stft(padded_waveform)\n",
    "print(\"stft_features shape:\", stft_features.shape)\n",
    "\n",
    "# Pass through the encoder\n",
    "encoder_out = model._model.encoder(stft_features)\n",
    "print(\"encoder_out shape:\", encoder_out.shape)\n",
    "\n",
    "# Initialize decoder state\n",
    "decoder = model._model.decoder\n",
    "state = (torch.zeros(encoder_out.size(0), decoder.hidden_size),\n",
    "         torch.zeros(encoder_out.size(0), decoder.hidden_size))\n",
    "print(\"Decoder hidden size:\", decoder.hidden_size)\n",
    "\n",
    "# Process through the decoder\n",
    "predictions = []\n",
    "for t in range(encoder_out.size(2)):\n",
    "    input_t = encoder_out[:, :, t]\n",
    "    print(\"input_t shape:\", input_t.shape)\n",
    "    h, c = decoder.rnn(input_t, state)\n",
    "    state = (h, c)\n",
    "    x = h.unsqueeze(-1)  # Shape: [batch_size, hidden_size, 1]\n",
    "    print(\"x shape:\", x.shape)\n",
    "    out = decoder.decoder(x)  # Apply the decoder layers\n",
    "    print(\"out shape:\", out.shape)\n",
    "    predictions.append(out)\n",
    "\n",
    "# Concatenate predictions\n",
    "predictions = torch.cat(predictions, dim=2)\n",
    "print(\"predictions shape:\", predictions.shape)\n",
    "\n",
    "# Get the speech probability for the first time step\n",
    "speech_prob = predictions[0, 0, 0].item()\n",
    "print(\"speech_prob:\", speech_prob)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "a5afc3b3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RecursiveScriptModule(\n",
       "  original_name=VADDecoderRNNJIT\n",
       "  (rnn): RecursiveScriptModule(original_name=LSTMCell)\n",
       "  (decoder): RecursiveScriptModule(\n",
       "    original_name=Sequential\n",
       "    (0): RecursiveScriptModule(original_name=Dropout)\n",
       "    (1): RecursiveScriptModule(original_name=ReLU)\n",
       "    (2): RecursiveScriptModule(original_name=Conv1d)\n",
       "    (3): RecursiveScriptModule(original_name=Sigmoid)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d888b094",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "aaml",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
