{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/timur.bikbulatov/personal/aa_on_vad\n",
      "/home/timur.bikbulatov/personal/aa_on_vad\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/timur.bikbulatov/miniconda3/envs/aaml/lib/python3.11/site-packages/IPython/core/magics/osm.py:417: UserWarning: This is now an optional IPython functionality, setting dhist requires you to install the `pickleshare` library.\n",
      "  self.shell.db['dhist'] = compress_dhist(dhist)[-100:]\n"
     ]
    }
   ],
   "source": [
    "%cd ../\n",
    "!pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.graph_objects as go\n",
    "import numpy as np\n",
    "\n",
    "def plot(y:list):\n",
    "    trace = []\n",
    "    colors = [\n",
    "        'Blue',\n",
    "        'Orange',\n",
    "        'Green',\n",
    "        'Red',\n",
    "        'Purple',\n",
    "        'Magenta',\n",
    "        'Cyan',\n",
    "        'Brown',\n",
    "        'Pink',\n",
    "        'Lime',\n",
    "        'Yellow',\n",
    "        'Teal',\n",
    "        'Olive',\n",
    "        'Navy',\n",
    "        'Maroon',\n",
    "        'Coral',\n",
    "        'Gold',\n",
    "        'Indigo',\n",
    "        'Turquoise',\n",
    "        'Lavender',\n",
    "        'Mint',\n",
    "        'Silver',\n",
    "        ]\n",
    "    for ik, y_ in enumerate(y):\n",
    "        trace.append(go.Scatter(x=np.arange(len(y_)), y=y_, mode='lines', name=f'arg # {ik + 1}', line=dict(color=colors[ik])))\n",
    "\n",
    "    # Combining both traces into one figure\n",
    "    fig = go.Figure(data=trace)\n",
    "\n",
    "    # Setting the layout\n",
    "    fig.update_layout(\n",
    "        title='Two Line Charts on One Plot',\n",
    "        xaxis_title='X-axis',\n",
    "        yaxis_title='Y-axis',\n",
    "        showlegend=True\n",
    "    )\n",
    "\n",
    "    # Display the plot\n",
    "    fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.evaluation.evaluator import VADEvaluator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.datasets.mixed_vad_datset import get_datset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "import torch\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def get_vad_mask(\n",
    "    audio: torch.Tensor,\n",
    "    model,\n",
    "    threshold: float = 0.5,\n",
    "    sample_rate: int = 16000,\n",
    "    window_size_samples: int = 512\n",
    ") -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    Convert VAD model predictions into a binary mask.\n",
    "    \n",
    "    Args:\n",
    "        audio: torch.Tensor - Input audio (1D tensor)\n",
    "        model: VAD model\n",
    "        threshold: float - Speech probability threshold\n",
    "        sample_rate: int - Audio sampling rate\n",
    "        window_size_samples: int - Window size for processing\n",
    "        \n",
    "    Returns:\n",
    "        torch.Tensor - Binary mask of same length as input audio\n",
    "    \"\"\"\n",
    "    # Ensure audio is 1D\n",
    "    if not torch.is_tensor(audio):\n",
    "        audio = torch.tensor(audio)\n",
    "    audio = audio.squeeze()\n",
    "    \n",
    "    # Handle sample_rate\n",
    "    if sample_rate > 16000 and (sample_rate % 16000 == 0):\n",
    "        step = sample_rate // 16000\n",
    "        sample_rate = 16000\n",
    "        audio = audio[::step]\n",
    "    \n",
    "    # Reset model states\n",
    "    if hasattr(model, 'reset_states'):\n",
    "        model.reset_states()\n",
    "    \n",
    "    # Initialize mask\n",
    "    audio_length = len(audio)\n",
    "    mask = torch.zeros(audio_length)\n",
    "    # Process audio in windows\n",
    "    for start_idx in range(0, audio_length, window_size_samples):\n",
    "        # Get chunk\n",
    "        chunk = audio[start_idx: start_idx + window_size_samples]\n",
    "        \n",
    "        # Pad last chunk if needed\n",
    "        if len(chunk) < window_size_samples:\n",
    "            chunk = F.pad(chunk, (0, window_size_samples - len(chunk)))\n",
    "        \n",
    "        # Get prediction\n",
    "        speech_prob = model(chunk, sample_rate).item()\n",
    "        \n",
    "        # Fill mask for this window\n",
    "        end_idx = min(start_idx + window_size_samples, audio_length)\n",
    "        mask[start_idx:end_idx] = float(speech_prob >= threshold)\n",
    "    \n",
    "    return mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "from tqdm import tqdm\n",
    "\n",
    "def validate_silero_vad(model, dataset, device):\n",
    "    \"\"\"\n",
    "    Validate Silero VAD model using the VADEvaluator\n",
    "    \n",
    "    Args:\n",
    "        model: Silero VAD model instance\n",
    "        dataset: Dataset instance providing audio samples and labels\n",
    "        device: torch device\n",
    "        batch_size: batch size for DataLoader\n",
    "    \"\"\"\n",
    "    dataloader = DataLoader(dataset, batch_size=1, shuffle=False)\n",
    "    evaluator = VADEvaluator(threshold=0.5)\n",
    "    sample_rate = 16000\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(dataloader):\n",
    "            # Move data to device\n",
    "            wavs = batch['sample'].to(device)\n",
    "            masks_true = batch['mask'].to(device)\n",
    "            \n",
    "            # Process each audio in batch\n",
    "            mask_pred = get_vad_mask(\n",
    "                wavs, \n",
    "                model, \n",
    "                sample_rate=sample_rate)\n",
    "            \n",
    "            # Update evaluator\n",
    "            evaluator.update(mask_pred, masks_true)\n",
    "    \n",
    "    # Compute and return metrics\n",
    "    metrics = evaluator.compute()\n",
    "    return metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in /home/timur.bikbulatov/.cache/torch/hub/snakers4_silero-vad_master\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "torch.set_num_threads(1)\n",
    "\n",
    "model, utils = torch.hub.load(repo_or_dir='snakers4/silero-vad', model='silero_vad')\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model = model.to(device)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create dataset\n",
    "dataset = get_datset(mode='test', erase_silence=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/timur.bikbulatov/personal/aa_on_vad/src/datasets/urbansound.py:27: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  audio = torch.tensor(self.audios[idx], dtype=torch.float32)\n"
     ]
    }
   ],
   "source": [
    "sample = dataset[0]\n",
    "wav = sample['sample']\n",
    "maskT = sample['mask']\n",
    "mask = get_vad_mask(wav.to('cuda:0'), model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot([wav, mask, maskT])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/262 [00:00<?, ?it/s]/home/timur.bikbulatov/personal/aa_on_vad/src/datasets/urbansound.py:27: UserWarning:\n",
      "\n",
      "To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "\n",
      "100%|██████████| 262/262 [00:12<00:00, 21.79it/s]\n"
     ]
    }
   ],
   "source": [
    "# Run validation\n",
    "metrics = validate_silero_vad(model, dataset, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation metrics:\n",
      "precision: 0.8470\n",
      "recall: 0.9164\n",
      "f1_score: 0.8803\n",
      "accuracy: 0.8978\n",
      "false_positive_rate: 0.1151\n",
      "true_positive_rate: 0.9164\n"
     ]
    }
   ],
   "source": [
    "print(\"Validation metrics:\")\n",
    "for metric_name, value in metrics.items():\n",
    "    print(f\"{metric_name}: {value:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "aaml",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
